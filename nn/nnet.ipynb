{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "np.random.seed(1337) # reproducibility\n",
    "\n",
    "def binarize(spk_labels):\n",
    "    # Binarize the output (one hot encoding of spk truth labels)\n",
    "    print(\"Binarizing labels...\")\n",
    "    bin_spk_labels = label_binarize(spk_labels, classes=list(set(spk_labels)))\n",
    "    return bin_spk_labels\n",
    "\n",
    "# because sklearn 0.17 no longer has sklearn.model_selection.train_test_split\n",
    "# NO LONGER USED BECAUSE KERAS MODEL FIT DOES VALIDATION SPLITS FOR YOU\n",
    "def dataset_split(X, y, test_size=0.1):\n",
    "    num_els = np.shape(X)[0]\n",
    "    assert num_els == np.shape(y)[0]\n",
    "    assert test_size <= 1 and test_size >= 0\n",
    "    \n",
    "    print(\"Splitting into train/test. Test proportion:\", test_size)\n",
    "    idxs = np.random.choice(np.arange(num_els), int((1-test_size)*num_els), replace=False)\n",
    "    inv_idxs = list(set(np.arange(num_els)) - set(idxs))\n",
    "    X_train, y_train, X_test, y_test = X[idxs], y[idxs], X[inv_idxs], y[inv_idxs]\n",
    "    \n",
    "    print(\"X,y train shapes\", X_train.shape, y_train.shape, \"X,y test shapes\", X_test.shape, y_test.shape)\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    " # all utterance data [utt1 mfcc frames [N1 x 60], utt2 frames]\n",
    "def mkdir(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enroll X shape (13685, 7200)\n",
      "Enroll y shape (13685,)\n"
     ]
    }
   ],
   "source": [
    "N_INP_FRMS = 120\n",
    "MODEL_PATH = 'model_' + str(N_INP_FRMS) + '/'\n",
    "mkdir(MODEL_PATH)\n",
    "BASE_PATH = \"/home/skoppula/biometrics/data/yoho/kaldi_yoho/data/\"\n",
    "VER_PATH = BASE_PATH + \"verify/final/nn_inp-\" + str(N_INP_FRMS) + \"_frames/\"\n",
    "ENR_PATH = BASE_PATH + \"enroll/final/nn_inp-\" + str(N_INP_FRMS) + \"_frames/\"\n",
    "\n",
    "\n",
    "ENR_X = np.load(ENR_PATH + \"X.npy\")\n",
    "ENR_y = np.load(ENR_PATH + \"y.npy\")\n",
    "\n",
    "NUM_FRAMES = np.shape(ENR_X)[0]\n",
    "ENR_X = ENR_X.reshape(NUM_FRAMES,N_INP_FRMS * 60) # 60 MFCCs per frame\n",
    "assert NUM_FRAMES == np.shape(ENR_y)[0]\n",
    "\n",
    "print(\"Enroll X shape\", np.shape(ENR_X))\n",
    "print(\"Enroll y shape\", np.shape(ENR_y))\n",
    "\n",
    "poss_spks = np.load(VER_PATH + \"y.npy\")\n",
    "\n",
    "curr_spk = 109\n",
    "model = get_ver_network_arch()\n",
    "tr_x, tr_y = ENR_X, conv_to_ver_and_one_hot_encode(ENR_y, curr_spk)\n",
    "\n",
    "\n",
    "# need spk in verify set (for now) to test positive authentication\n",
    "# VER_y_60 = np.load(\"/home/skoppula/biometrics/data/yoho/kaldi_yoho/data/verify/final/nn_inp-60_frames/y.npy\")\n",
    "# curr_spk = random.choice(ENR_y_60)\n",
    "# while curr_spk not in VER_y_60:\n",
    "#   curr_spk = print(random.choice(ENR_y_60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_verify_lbls(y, curr_spk):\n",
    "    return np.array(list(map(lambda x: 1 if x == curr_spk else 0, y)))\n",
    "\n",
    "def conv_to_ver_and_one_hot_encode(y, spk):\n",
    "    # convert to verification task\n",
    "    ver_lbls = get_verify_lbls(y, curr_spk)\n",
    "    num_frames = np.shape(y)[0]\n",
    "    \n",
    "    one_hot_lbls = np.zeros((num_frames, 2))\n",
    "    one_hot_lbls[np.arange(num_frames), ver_lbls] = 1\n",
    "    return one_hot_lbls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils.visualize_util import plot\n",
    "from keras.layers.core import K\n",
    "\n",
    "def get_ver_network_arch():\n",
    "    # Add batch normalization: keras.layers.normalization.BatchNormalization()\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_shape=(N_INP_FRMS*60,))) # 60 MFCCs / frame\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(16))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    model.summary()\n",
    "    plot(model, to_file=MODEL_PATH + 'architecture.png')\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(),\n",
    "                  metrics=['accuracy', 'precision', 'recall'])\n",
    "    return model\n",
    "\n",
    "def evaluate_activations(model, X, layer):\n",
    "    get_layer_output = K.function([model.layers[0].input, K.learning_phase()], [model.layers[layer].output])\n",
    "    return get_layer_output([X, 0])[0]\n",
    "\n",
    "def train_and_test_network(model, tr_x, tr_y, curr_spk, MODEL_PATH):\n",
    "    \n",
    "    EPOCHS = 2\n",
    "    BATCH_SIZE = 50\n",
    "    \n",
    "    saved_model_path = MODEL_PATH + str(curr_spk) + \"_curr_best_weights.hdf5\"\n",
    "    ckpt = ModelCheckpoint(saved_model_path, monitor='val_acc', verbose=1, save_best_only=False, mode='max')\n",
    "    trn_history = model.fit(tr_x, tr_y, validation_split=0.2,\n",
    "                        batch_size=BATCH_SIZE, nb_epoch=EPOCHS, verbose=1, \n",
    "                        callbacks=[ckpt])\n",
    "    \n",
    "    model.load_weights(saved_model_path)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(),\n",
    "                  metrics=['accuracy', 'precision', 'recall'])\n",
    "\n",
    "    te_x = np.load(VER_PATH + \"X.npy\")\n",
    "    NUM_TEST_FRAMES = np.shape(te_x)[0]\n",
    "    te_x = te_x.reshape(NUM_TEST_FRAMES,N_INP_FRMS * 60)\n",
    "    te_y = conv_to_ver_and_one_hot_encode(np.load(VER_PATH + \"y.npy\"), curr_spk)\n",
    "    assert NUM_TEST_FRAMES == np.shape(te_y)[0]\n",
    "\n",
    "    print(\"Verify X shape\", np.shape(te_x))\n",
    "    print(\"Verify y shape\", np.shape(te_y))\n",
    "    \n",
    "    # del tr_x, tr_y, val_x, val_y # for saving memory\n",
    "    score = model.evaluate(te_x, te_y, verbose=0)\n",
    "    print('Test score:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "\n",
    "    activations = get_activations(model, te_x, 2)\n",
    "    np.save(MODEL_PATH + \"activations_\" + str(curr_spk) + \".npy\", activations)\n",
    "    np.save(MODEL_PATH + \"history_\" + str(curr_spk) + \".npy\", trn_history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.135967777725\n",
      "Test accuracy: 0.99064171123\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': [0.98885641622421383, 0.99104859780385457],\n",
       " 'loss': [0.15564804439031557, 0.14193820062563922],\n",
       " 'precision': [0.98871237883438889, 0.99104859780385457],\n",
       " 'recall': [0.98885641622421383, 0.99104859780385457],\n",
       " 'val_acc': [1.0, 1.0],\n",
       " 'val_loss': [0.0008760287819792531, 0.00039988009035069923],\n",
       " 'val_precision': [1.0, 1.0],\n",
       " 'val_recall': [1.0, 1.0]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
