{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# all utterance data [utt1 mfcc frames [N1 x 60], utt2 frames]\n",
    "def mkdir(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "def get_verify_lbls(y, curr_spk):\n",
    "    return np.array(list(map(lambda x: 1 if x == curr_spk else 0, y)))\n",
    "\n",
    "def conv_to_ver_and_one_hot_encode(y, curr_spk):\n",
    "    # convert to verification task\n",
    "    ver_lbls = get_verify_lbls(y, curr_spk)\n",
    "    num_frames = np.shape(y)[0]\n",
    "\n",
    "    one_hot_lbls = np.zeros((num_frames, 2))\n",
    "    one_hot_lbls[np.arange(num_frames), ver_lbls] = 1\n",
    "    return one_hot_lbls\n",
    "\n",
    "def get_ver_network_arch(model_path, n_inp_frms):\n",
    "\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers.core import Dense, Dropout, Activation\n",
    "    from keras.utils.visualize_util import plot\n",
    "    from keras.optimizers import Adam\n",
    "\n",
    "    # Add batch normalization: keras.layers.normalization.BatchNormalization()\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_shape=(n_inp_frms*60,))) # 60 MFCCs / frame\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(16))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.summary()\n",
    "    plot(model, to_file=model_path + 'architecture.png')\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(),\n",
    "                  metrics=['accuracy', 'precision', 'recall'])\n",
    "    return model\n",
    "\n",
    "def evaluate_activations(model, X, layer):\n",
    "    from keras.layers.core import K\n",
    "    get_layer_output = K.function([model.layers[0].input, K.learning_phase()], [model.layers[layer].output])\n",
    "    return get_layer_output([X, 0])[0]\n",
    "\n",
    "def train_and_test_network(model, tr_x, tr_y, te_x, te_y, curr_spk, model_path, n_epochs=75, batch_size=50):\n",
    "\n",
    "    from keras.optimizers import Adam\n",
    "    from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "    saved_model_path = model_path + \"curr_best_weights_\"  + str(curr_spk) + \".hdf5\"\n",
    "    ckpt = ModelCheckpoint(saved_model_path, monitor='val_acc', verbose=0, save_best_only=False, mode='max')\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=0, verbose=1, mode='auto')\n",
    "    trn_history = model.fit(tr_x, tr_y, validation_split=0.2,\n",
    "                        batch_size=batch_size, nb_epoch=n_epochs, verbose=1,\n",
    "                        callbacks=[ckpt, early_stop])\n",
    "\n",
    "    np.save(model_path + \"history_\" + str(curr_spk) + \".npy\", trn_history.history)\n",
    "\n",
    "    model.load_weights(saved_model_path)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(),\n",
    "                  metrics=['accuracy', 'precision', 'recall'])\n",
    "\n",
    "    # del tr_x, tr_y, val_x, val_y # for saving memory\n",
    "    score = model.evaluate(te_x, te_y, verbose=0)\n",
    "    print('Test score:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "\n",
    "    gc.collect() # fix suggested by https://github.com/tensorflow/tensorflow/issues/3388\n",
    "\n",
    "    model.load_weights(saved_model_path)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(),\n",
    "                  metrics=['accuracy', 'precision', 'recall'])\n",
    "\n",
    "    activations = evaluate_activations(model, te_x, 7)\n",
    "    np.save(model_path + \"activations_\" + str(curr_spk) + \".npy\", activations)\n",
    "\n",
    "    return score[1]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(1337) # reproducibility\n",
    "\n",
    "    for N_INP_FRMS in [60, 80, 120]:\n",
    "        print(\"\\nNUM INPUT FRAMES:\",N_INP_FRMS,\"\\n\")\n",
    "        MODEL_PATH = 'model_' + str(N_INP_FRMS) + '/'\n",
    "        mkdir(MODEL_PATH)\n",
    "        BASE_PATH = \"/home/skoppula/biometrics/data/yoho/kaldi_yoho/data/\"\n",
    "        VER_PATH = BASE_PATH + \"verify/final/nn_inp-\" + str(N_INP_FRMS) + \"_frames/\"\n",
    "        ENR_PATH = BASE_PATH + \"enroll/final/nn_inp-\" + str(N_INP_FRMS) + \"_frames/\"\n",
    "\n",
    "        print(\"Model(s) Path: \", MODEL_PATH)\n",
    "        print(\"Using verification path: \", VER_PATH)\n",
    "        print(\"Using enroll data path: \", ENR_PATH)\n",
    "\n",
    "        # ENROLL DATA LOAD\n",
    "        enr_x = np.load(ENR_PATH + \"X.npy\")\n",
    "        n_frames = np.shape(enr_x)[0]\n",
    "        enr_x = enr_x.reshape(n_frames, N_INP_FRMS * 60) # 60 MFCCs per frame\n",
    "        enr_y = np.load(ENR_PATH + \"y.npy\")\n",
    "        assert n_frames == np.shape(enr_y)[0]\n",
    "        print(\"Enroll X shape\", np.shape(enr_x))\n",
    "        print(\"Enroll y shape\", np.shape(enr_y))\n",
    "\n",
    "        # VERIFY DATA LOAD\n",
    "        ver_x = np.load(VER_PATH + \"X.npy\")\n",
    "        n_frames = np.shape(ver_x)[0]\n",
    "        ver_x = ver_x.reshape(n_frames, N_INP_FRMS * 60)\n",
    "        ver_y = np.load(VER_PATH + \"y.npy\")\n",
    "        assert n_frames == np.shape(ver_y)[0]\n",
    "        print(\"Verify X shape\", np.shape(ver_x))\n",
    "        print(\"Verify y shape\", np.shape(ver_y))\n",
    "\n",
    "        poss_spks = list(set(np.load(VER_PATH + \"y.npy\")))\n",
    "\n",
    "        test_accs = []\n",
    "        for i, curr_spk in enumerate(poss_spks):\n",
    "            print(\"EVALUATING AND TRAINING FOR CURRENT SPEAKER:\", curr_spk, str(i) + \"/\" + str(len(poss_spks)))\n",
    "            model = get_ver_network_arch(MODEL_PATH, N_INP_FRMS)\n",
    "            tr_y = conv_to_ver_and_one_hot_encode(enr_y, curr_spk)\n",
    "            te_y = conv_to_ver_and_one_hot_encode(ver_y, curr_spk)\n",
    "            spk_path = MODEL_PATH + str(curr_spk) + \"/\"\n",
    "            mkdir(spk_path)\n",
    "            test_acc = train_and_test_network(model, enr_x, tr_y, ver_x, te_y, curr_spk, spk_path)\n",
    "            test_accs.append(test_acc)\n",
    "\n",
    "        gc.collect()\n",
    "        print(\"TEST ACCURACIES:\",test_accs)\n",
    "        print(\"Final average test accuracy:\", sum(test_accs)/len(test_accs))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
